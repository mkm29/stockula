# syntax=docker/dockerfile:1
# GPU-enabled Dockerfile for Stockula trading platform with NVIDIA CUDA support
#
# This Dockerfile extends the base Stockula image with GPU acceleration for
# time series forecasting using AutoTS with PyTorch, TensorFlow, and GluonTS.
#
# Requirements:
#   - NVIDIA Container Runtime (nvidia-docker2)
#   - NVIDIA GPU with CUDA capability >= 3.7
#   - Host system with NVIDIA drivers >= 450.80.02
#
# Build stages:
#   - nvidia-base: NVIDIA CUDA base with system dependencies
#   - gpu-dependencies: Python packages with GPU support
#   - gpu-source: Application source code
#   - gpu-production: Minimal runtime image with GPU support
#   - gpu-cli: GPU production + interactive tools
#
# Usage:
#   docker buildx build -f Dockerfile.nvidia --target gpu-production -t stockula:gpu-prod .
#   docker buildx build -f Dockerfile.nvidia --target gpu-cli -t stockula:gpu-cli .
#   docker run --gpus all stockula:gpu-cli

# Stage 1: NVIDIA CUDA base image with uv
FROM nvidia/cuda:13.0.0-devel-ubuntu24.04 AS nvidia-base

# Build arguments for labels
ARG VERSION="0.0.0"
ARG BUILD_DATE
ARG GIT_COMMIT
ARG GIT_URL

# Use bash with pipefail option for better error handling
SHELL ["/bin/bash", "-o", "pipefail", "-c"]

# Set environment variables for CUDA and Python
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    CUDA_VERSION=13.0 \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    UV_SYSTEM_PYTHON=1 \
    UV_COMPILE_BYTECODE=1

# Install Python 3.12, uv, and system dependencies
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    # Python 3.12 (better ML library compatibility)
    python3.12 \
    python3.12-dev \
    python3.12-venv \
    # Build dependencies
    build-essential \
    curl \
    git \
    pkg-config \
    # Additional dependencies for ML libraries
    libhdf5-dev \
    libffi-dev \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Install uv for Python 3.12
RUN curl -LsSf https://astral.sh/uv/install.sh | sh
ENV PATH="/root/.local/bin:$PATH"

# Create Python 3.12 symlink for compatibility
RUN ln -sf /usr/bin/python3.12 /usr/bin/python3 && \
    ln -sf /usr/bin/python3.12 /usr/bin/python

# Stage 2: GPU Dependencies builder (cache GPU-enabled Python packages)
FROM nvidia-base AS gpu-dependencies

WORKDIR /app

# Copy dependency files and README (required by pyproject.toml)
COPY pyproject.toml uv.lock README.md ./
COPY src/ src/

# Create virtual environment and install base dependencies
# Note: We manually install dependencies to bypass Python version check
RUN --mount=type=cache,target=/root/.cache/uv,sharing=locked \
    uv venv /opt/venv --python 3.12 && \
    # Install pip in the venv using ensurepip
    /opt/venv/bin/python -m ensurepip --upgrade && \
    /opt/venv/bin/python -m pip install --upgrade pip && \
    # Install core dependencies manually (from pyproject.toml)
    /opt/venv/bin/pip install \
        "pandas>=2.0.0" \
        "numpy>=1.24.0" \
        "yfinance>=0.2.33" \
        "scikit-learn>=1.3.0" \
        "scipy>=1.10.0" \
        "statsmodels>=0.14.0" \
        "sqlmodel>=0.0.16" \
        "alembic>=1.13.0" \
        "pydantic>=2.6.0" \
        "pydantic-settings>=2.2.0" \
        "python-dotenv>=1.0.0" \
        "rich>=13.7.0" \
        "typer>=0.9.0" \
        "click>=8.1.7" \
        "pyyaml>=6.0" \
        "dependency-injector>=4.41.0" \
        "autots>=0.6.14" \
        "backtesting>=0.6.4" \
        "finta>=1.3" \
        "curl-cffi>=0.12.0" && \
    # Add stockula to Python path instead of installing (to avoid version check)
    echo "/app/src" > /opt/venv/lib/python3.12/site-packages/stockula.pth

# Install GPU-specific packages for AutoTS acceleration
# Using CUDA 11.8 packages for better compatibility with current ecosystem
RUN --mount=type=cache,target=/root/.cache/pip,sharing=locked \
    # PyTorch with CUDA 11.8 support (most stable)
    /opt/venv/bin/pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 && \
    # TensorFlow with GPU support (2.16+ for Python 3.12)
    /opt/venv/bin/pip install tensorflow[and-cuda]==2.16.* && \
    # Core GPU-accelerated time series libraries
    /opt/venv/bin/pip install neuralforecast && \
    /opt/venv/bin/pip install pytorch-forecasting && \
    # GluonTS for time series forecasting (CPU version for compatibility)
    /opt/venv/bin/pip install gluonts[torch] && \
    # XGBoost with GPU support
    /opt/venv/bin/pip install xgboost && \
    # LightGBM (standard version, GPU will be detected at runtime)
    /opt/venv/bin/pip install lightgbm

# Stage 3: GPU Source builder
FROM gpu-dependencies AS gpu-source

# Copy additional source files (src already copied in dependencies stage)
COPY alembic.ini ./
COPY alembic/ alembic/
COPY examples/ examples/
COPY data/ data/

# Package is already installed in dependencies stage, just verify
RUN . /opt/venv/bin/activate && python -c "import stockula; print(f'Stockula {stockula.__version__} installed')"

# Stage 4: GPU Production runtime - optimized for GPU inference
FROM nvidia/cuda:13.0.0-runtime-ubuntu24.04 AS gpu-production

# Re-declare build arguments for this stage
ARG VERSION=dev
ARG BUILD_DATE
ARG GIT_COMMIT
ARG GIT_URL

# Set GPU-specific environment variables
ENV NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    CUDA_VERSION=13.0 \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install minimal runtime dependencies including Python 3.12
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-venv \
    ca-certificates \
    libgomp1 \
    && ln -sf /usr/bin/python3.12 /usr/bin/python3 \
    && ln -sf /usr/bin/python3.12 /usr/bin/python \
    && rm -rf /var/lib/apt/lists/*

# Use existing ubuntu user (UID 1000) and rename to stockula for consistency
RUN usermod -l stockula ubuntu && \
    groupmod -n stockula ubuntu && \
    usermod -d /home/stockula -m stockula

WORKDIR /app

# Copy virtual environment and source from builder
COPY --from=gpu-source --chown=stockula:stockula /opt/venv /opt/venv
COPY --from=gpu-source --chown=stockula:stockula /app /app

# Set up environment
ENV VIRTUAL_ENV=/opt/venv \
    PATH="/opt/venv/bin:$PATH" \
    STOCKULA_VERSION="${VERSION}" \
    # AutoTS GPU-specific settings
    AUTOTS_GPU_ENABLED=true \
    # Reduce job count for stability as recommended by AutoTS
    AUTOTS_N_JOBS=1 \
    # TensorFlow GPU settings
    TF_FORCE_GPU_ALLOW_GROWTH=true \
    # PyTorch settings
    TORCH_CUDA_ARCH_LIST="6.0 6.1 7.0 7.5 8.0 8.6 9.0+PTX"

# Create data directories with proper permissions
RUN mkdir -p /app/data /app/results /app/models && \
    chown -R stockula:stockula /app

USER stockula

VOLUME ["/app/data", "/app/results", "/app/models"]

# GPU-aware health check
HEALTHCHECK --interval=30s --timeout=15s --start-period=120s --retries=3 \
    CMD python -c "import stockula; import torch; print(f'Stockula GPU Health: CUDA Available={torch.cuda.is_available()}, Device Count={torch.cuda.device_count()}')" || exit 1

# Expose metadata about the image
EXPOSE 8888/tcp

# Default entrypoint and command
ENTRYPOINT ["python", "-m"]
CMD ["stockula", "--help"]

# Stage 5: GPU CLI stage - optimized for interactive GPU development
FROM gpu-production AS gpu-cli

# Add stage-specific label
LABEL com.stockula.build.stage="gpu-cli" \
      org.opencontainers.image.description="Stockula GPU CLI - Interactive GPU-accelerated trading analysis with NVIDIA CUDA support"

USER root

# Install development and debugging tools
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    apt-get update && apt-get install -y --no-install-recommends \
    # Interactive tools
    less \
    nano \
    htop \
    # GPU debugging tools
    nvidia-utils-535 \
    && rm -rf /var/lib/apt/lists/*

# Install additional development packages in virtual environment
RUN /opt/venv/bin/pip install \
    jupyter \
    ipython \
    matplotlib \
    seaborn \
    plotly \
    # GPU profiling tools
    py3nvml \
    pynvml

USER stockula

# Create Jupyter configuration for GPU usage
RUN mkdir -p /home/stockula/.jupyter && \
    echo "c.NotebookApp.ip = '0.0.0.0'" >> /home/stockula/.jupyter/jupyter_notebook_config.py && \
    echo "c.NotebookApp.port = 8888" >> /home/stockula/.jupyter/jupyter_notebook_config.py && \
    echo "c.NotebookApp.open_browser = False" >> /home/stockula/.jupyter/jupyter_notebook_config.py && \
    echo "c.NotebookApp.allow_root = False" >> /home/stockula/.jupyter/jupyter_notebook_config.py

# Create a GPU info script for easy checking
RUN echo '#!/bin/bash' > /home/stockula/gpu_info.sh && \
    echo 'echo "=== GPU Information ==="' >> /home/stockula/gpu_info.sh && \
    echo 'python -c "import torch; print(f\"PyTorch CUDA: {torch.cuda.is_available()}, Devices: {torch.cuda.device_count()}\")' >> /home/stockula/gpu_info.sh && \
    echo 'python -c "import tensorflow as tf; print(f\"TensorFlow GPUs: {len(tf.config.list_physical_devices('"'"'GPU'"'"'))}\")' >> /home/stockula/gpu_info.sh && \
    echo 'nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv' >> /home/stockula/gpu_info.sh && \
    chmod +x /home/stockula/gpu_info.sh

# OCI Standard Labels with GPU-specific metadata
LABEL org.opencontainers.image.created="${BUILD_DATE}" \
      org.opencontainers.image.authors="Stockula Contributors <https://github.com/mkm29/stockula>" \
      org.opencontainers.image.url="https://github.com/mkm29/stockula" \
      org.opencontainers.image.documentation="https://github.com/mkm29/stockula/tree/main/docs" \
      org.opencontainers.image.source="${GIT_URL}" \
      org.opencontainers.image.version="${VERSION}" \
      org.opencontainers.image.revision="${GIT_COMMIT}" \
      org.opencontainers.image.vendor="Stockula Project" \
      org.opencontainers.image.licenses="MIT" \
      org.opencontainers.image.ref.name="${VERSION}" \
      org.opencontainers.image.title="Stockula GPU" \
      org.opencontainers.image.description="GPU-accelerated Stockula trading platform with NVIDIA CUDA support for time series forecasting"

# Custom Labels for GPU-specific metadata
LABEL com.stockula.python.version="3.12" \
      com.stockula.base.image="nvidia/cuda:13.0.0-runtime-ubuntu24.04" \
      com.stockula.build.stage="gpu-production" \
      com.stockula.gpu.support="true" \
      com.stockula.cuda.version="13.0" \
      com.stockula.maintainer="Mitch Murphy"


ENV SHELL=/bin/bash
CMD ["/bin/bash"]